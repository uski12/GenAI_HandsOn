{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Name:** Sidharth Baburaj  \n",
        "**SRN:** PES2UG23CS578  \n",
        "**Section:** 6-'I'\n",
        "\n",
        "**Assignment 3 -** Advanced Prompting"
      ],
      "metadata": {
        "id": "XvmQsesCehE7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiX-l5B96MZE",
        "outputId": "b5890fed-dfde-4134-9c1f-c6d76dd19678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m133.1/137.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your Groq API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b (Small/Fast) to demonstrate logic failures\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\"\n",
        "\n",
        "# 1. Standard Prompt (Direct Answer)\n",
        "prompt_standard = f\"Answer this question: {question}\"\n",
        "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_standard).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNi_L9LP7UlZ",
        "outputId": "0ff1b965-95eb-4691-bf2f-853a913e599c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STANDARD (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Roger has now, we need to add the initial number of tennis balls he had (5) to the number of tennis balls he bought (2 cans * 3 tennis balls per can).\n",
            "\n",
            "2 cans * 3 tennis balls per can = 6 tennis balls\n",
            "\n",
            "Now, let's add the initial number of tennis balls (5) to the number of tennis balls he bought (6):\n",
            "\n",
            "5 + 6 = 11\n",
            "\n",
            "So, Roger now has 11 tennis balls.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. CoT Prompt (Magic Phrase)\n",
        "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
        "\n",
        "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_cot).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZVYnspr7YF2",
        "outputId": "d7df4afb-3e51-4e0d-9f6a-5740250ee6e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chain of Thought (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Roger has now, we need to follow these steps:\n",
            "\n",
            "1. Roger already has 5 tennis balls.\n",
            "2. He buys 2 more cans of tennis balls. Each can has 3 tennis balls, so he buys 2 x 3 = 6 more tennis balls.\n",
            "3. Now, we add the tennis balls he already had (5) to the tennis balls he bought (6). 5 + 6 = 11\n",
            "\n",
            "So, Roger now has 11 tennis balls.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7) # Creativity needed"
      ],
      "metadata": {
        "id": "c6bD51HI_7qY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "problem = \"How can I get my 5-year-old to eat vegetables?\"\n",
        "\n",
        "# Step 1: The Branch Generator\n",
        "prompt_branch = ChatPromptTemplate.from_template(\n",
        "    \"Problem: {problem}. Give me one unique, creative solution. Solution {id}:\"\n",
        ")\n",
        "\n",
        "branches = RunnableParallel(\n",
        "    sol1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
        "    sol2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
        "    sol3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# Step 2: The Judge\n",
        "prompt_judge = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three proposed solutions for: '{problem}'\n",
        "\n",
        "    1: {sol1}\n",
        "    2: {sol2}\n",
        "    3: {sol3}\n",
        "\n",
        "    Act as a Child Psychologist. Pick the most sustainable one (not bribery) and explain why.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Chain: Input -> Branches -> Judge -> Output\n",
        "tot_chain = (\n",
        "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
        "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]})\n",
        "    | prompt_judge\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Tree of Thoughts (ToT) Result ---\")\n",
        "print(tot_chain.invoke(problem))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afeXm-jW_7s2",
        "outputId": "f1730109-6033-4939-d0e7-189da1f55759"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tree of Thoughts (ToT) Result ---\n",
            "Based on the proposed solutions, I would recommend **Solution 2: Create a \"Veggie Face\" on Their Plate**. Here's why:\n",
            "\n",
            "1. **Non-bribery approach**: Unlike Solution 1, which uses a hidden treasure approach that might be seen as a bribe, Solution 2 focuses on creative presentation and doesn't involve any external rewards. This approach is more sustainable and promotes healthy eating habits.\n",
            "2. **Encourages creativity and imagination**: By involving your child in the veggie face creation process, you're fostering their creativity and imagination. This can make mealtime more enjoyable and engaging for them.\n",
            "3. **Gradual introduction of new vegetables**: Solution 2 suggests starting with simple vegetables like cherry tomatoes and carrots, and gradually introducing more complex ones like broccoli and cauliflower. This approach helps your child become familiar with a variety of vegetables without feeling overwhelmed.\n",
            "4. **Collaborative effort**: By encouraging your child to help with the veggie face creation, you're promoting a collaborative effort and making mealtime a fun, interactive experience.\n",
            "5. **Promotes healthy eating habits**: By making mealtime enjoyable and engaging, you're more likely to encourage your child to try new vegetables and develop healthy eating habits.\n",
            "\n",
            "As a child psychologist, I would also suggest the following additional strategies to enhance the effectiveness of Solution 2:\n",
            "\n",
            "1. **Involve your child in the process**: Encourage your child to help with meal planning and grocery shopping. This can help them feel more invested in the meal and more willing to try new vegetables.\n",
            "2. **Use positive language**: Instead of saying \"eat your vegetables,\" try saying \"let's make a veggie face together.\" This can help create a positive association with vegetables and mealtime.\n",
            "3. **Be patient and consistent**: Developing a taste for vegetables takes time, so be patient and consistent in your efforts. Avoid forcing your child to eat vegetables if they're not interested, as this can create negative associations.\n",
            "\n",
            "Overall, Solution 2 offers a creative and sustainable approach to encouraging your child to eat vegetables, while promoting healthy eating habits and a positive relationship with food.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. The Generator (Divergence)\n",
        "prompt_draft = ChatPromptTemplate.from_template(\n",
        "    \"Write a 1-sentence movie plot about: {topic}. Genre: {genre}.\"\n",
        ")\n",
        "\n",
        "drafts = RunnableParallel(\n",
        "    draft_scifi=prompt_draft.partial(genre=\"Sci-Fi\") | llm | StrOutputParser(),\n",
        "    draft_romance=prompt_draft.partial(genre=\"Romance\") | llm | StrOutputParser(),\n",
        "    draft_horror=prompt_draft.partial(genre=\"Horror\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# 2. The Aggregator (Convergence)\n",
        "prompt_combine = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three movie ideas for the topic '{topic}':\n",
        "    1. Sci-Fi: {draft_scifi}\n",
        "    2. Romance: {draft_romance}\n",
        "    3. Horror: {draft_horror}\n",
        "\n",
        "    Your task: Create a new Mega-Movie that combines the TECHNOLOGY of Sci-Fi, the PASSION of Romance, and the FEAR of Horror.\n",
        "    Write one paragraph.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 3. The Chain\n",
        "got_chain = (\n",
        "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
        "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]})\n",
        "    | prompt_combine\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Graph of Thoughts (GoT) Result ---\")\n",
        "print(got_chain.invoke(\"Time Travel\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hccEeiq_7vT",
        "outputId": "35c8a741-94aa-4f35-c082-af65a9d9465e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Graph of Thoughts (GoT) Result ---\n",
            "Here's a new movie concept that combines the technology of Sci-Fi, the passion of Romance, and the fear of Horror:\n",
            "\n",
            "\"In 'Echoes of Eternity,' a brilliant and reclusive physicist, Emma Taylor, discovers a way to manipulate time by creating a device that can record and replay human memories. Her experiment goes awry when she inadvertently activates the device, unleashing a dark entity from a parallel timeline that begins to possess the minds of those closest to her, including her fiancé, Jack. As Emma desperately tries to comprehend the consequences of her actions and find a way to stop the entity, she finds herself falling for a mysterious stranger, Alexander, who seems to be from her own past. But as she delves deeper into the mysteries of time, she realizes that Alexander may not be who he seems, and that their love is entwined with the very fabric of time itself. Can Emma stop the entity and prevent a catastrophic future from unfolding, or will she become trapped in a never-ending cycle of love, loss, and horror?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5mUQCPJiPUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}